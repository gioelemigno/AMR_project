In this section, we will introduce all the theoretical fundamentals necessary to understand the theory behind the Hamilton-Jacobi Reachability Analysis.

\subsection{Reachability Analysis}
The goal of reachability analysis is to compute the reach-avoid set (RAS) defined as the set of initial states from which the system, using an optimal input, can be driven to a target set within a finite time horizon and satisfying time-varying state constraints at all times. In a reachability problem formulation, the target set can represent a set of undesired states (unsafe), or a set of desired states, in the first case, the RAS contains states to avoid since there exists an optimal input (disturbance) that leads the state into an unsafe region. In the second case instead, the RAS represents safe states from which, applying an optimal input (control law), the system can reach a desired state. 
In the literature, the just described reach-avoid set, is also called capture basin \cite{new_paper} or backward reachable set (BRS) \cite{brief_intro}. In alternative to it, in some cases one might be interested in computing a forward reachable set (FRS), defined as the set of all states that the system can reach from a given initial set of states after a finite time horizon. To understand the difference between BRS and FRS, consider a reachability problem in which the target set contains unsafe states, the FRS can be used to check whether the set of possible future states of the system includes undesired states, the BRS instead, can be used to compute, by starting from known unsafe conditions, those initial states that must be avoided. In this paper HJ-RA is used to compute the BRS of the system, however, it can be used also to compute the FRS. 
All systems in the real world are subject to a disturbance, hence they have two different kinds of inputs, a controllable one $u$ (control) and another uncontrollable $d$ (disturbance); for that reason the computation of the BRS can be formulated in terms of a two-player game. For instance, consider an aircraft that has to follow a trajectory to complete a task, the system has two inputs: a control input (Player 1) and a disturbance (Player 2), in this scenario, the disturbance could be the wind. Suppose now that there is a goal position to reach (target set) along the trajectory, therefore the control input tries to bring the state at the target and the disturbance to steer it away, in this case, the BRS contains all the initial states for which exists an optimal control command that despite the worst disturbance, brings the system at the goal position. Suppose instead of a goal, there is an obstacle along the trajectory, now the target set is defined by all states of the system that correspond to a collision with the obstacle, therefore, the BRS contains those states which could lead to a collision despite the best possible control action. In both cases, the BRS can be computed by studying the outcome of the game between the two players. Due to the way they are formulated, those two games are “games of kind”, namely games in which the outcome is binary: system reaches or not the target set. In order to solve this type of game, it is necessary to translate it into a “games of degree” in which players want to optimize a cost function $J(\cdot)$, with opposite goals: one tries to maximize and the other to minimize it. An approach we will see later, called Level Set Method, can perform this kind of transformation by translating the problem into a standard differential game.

\subsection{Differential Games}
In this section we will introduce the basic concepts of the game theory related to a two person, zero-sum differential game, namely a mathematical representation in game theory of a situation which involves two sides, where the result is an advantage for one side and a loss for the other. If the total gains of the participants are added up, and the total losses are subtracted, they will sum to zero.
\newline
\subsubsection{Definition of the differential game}
Consider the system dynamics $\dot{x}=f(x(s), u(s), d(s))$ where $s \in [t, T]$ represents the time variable, $x(\cdot)\in\mathbb{R}^n$, $u(\cdot)\in U\subseteq \mathbb{R}^m$, $d(\cdot)\in D\subseteq \mathbb{R}^p$, $f:\mathbb{R}^n \times U \times D \rightarrow \mathbb{R}^n$. We use $s$ as the time variable to agree with the standard notation in game theory in which the game starts at time $s=t$, with $t \in [0, T]$, and ends at $s=T$ \cite{evans}.
The inputs $u(\cdot)$, $d(\cdot)$ represent Player 1 (control) and Player 2 (disturbance) respectively, we assume that they are drawn from the set of measurable functions \cite{evans}\cite{brief_intro}:
\[ 
    u(\cdot) \in \mathcal{U}_{[t, T]}  \triangleq 
    \left\{
        \sigma: [t, T] \rightarrow U| \sigma(\cdot) \, \textrm{is measurable} \, 
    \right\} 
\]
\[ 
    d(\cdot) \in \mathcal{D}_{[t, T]}  \triangleq 
    \left\{
        \sigma: [t, T] \rightarrow D| \sigma(\cdot) \, \textrm{is measurable} \, 
    \right\} 
\]
Assume $U$, $D$ are compact, $f(\cdot)$ is bounded and Lipschitz continuous in $x(\cdot)$ and continuous in $u(\cdot)$ and $d(\cdot)$, therefore, the system dynamics admits an unique trajectory $x(\cdot)$ that represents the response of the system to the inputs $u(\cdot)$, $d(\cdot)$ starting from an initial state $x(t)=x$.
The differential equation associated to the game is then the following one:
\begin{equation}
	\label{ode}
	\left\{
		\begin{array}{ll}
			\dot{x}(s)=f(x(s), u(s), d(s))  & s \in [t, T] \\
			x(t) = x & t \in [0, T]
		\end{array}
	\right.
\end{equation}

In order to complete the game definition is necessary to introduce the payoff function of the game $J(\cdot)$ and to do that we use two Lipschitz continuous functions $c:\mathbb{R}^n \rightarrow \mathbb{R}$ and $q:\mathbb{R}^n \rightarrow \mathbb{R}$:
\begin{equation}
	\label{payoff}
	J(x, t, u(\cdot), d(\cdot)) = \int_{t}^{T} c(x(s),d(s),u(s),s)  \,ds + q(x(T)) 
\end{equation}

The payoff function represents the reward/cost obtained by the two players at the end of the differential game (\ref{ode}) started at time $t$ with an initial state $x$, and subject to the commands  $u(\cdot)$, $d(\cdot)$ chosen by Player 1 and Player 2 respectively. The function $c(\cdot)$ indicates a running cost, therefore the first term of (\ref{payoff}) is the reward gained during the game (trajectory). The second term instead, uses $q(\cdot)$ to evaluate the final state $x(T)$ reached.
\newline

\subsubsection{Value of the game}
In order to solve the game, namely computes its outcome, it's necessary to define the goal and the information pattern of each player. Since we are referring to a zero-sum game, the aims of the players must be opposite, without loss of generality we assume Player 1 ($u(\cdot)$) wants to minimize the cost function $J(\cdot)$ and Player 2 ($d(\cdot)$) tries to maximize it. The information pattern of a game indicates which information a player has respect to its opponent.

In the case of "simple" games, usually there exists a dominant strategy for each player, namely an optimal strategy that is better than other ones independently to the opponent's strategy. In these cases it is easy to solve the game, since assuming rational players, we know a priori the strategies that will be chosen and therefore we can compute the game outcome in advance. In more complex games like the differential game (\ref{ode}) in which no longer exists dominant strategies, we cannot predict the outcome therefore we have to find another way to solve the game. For this reason in game theory two quantities called lower value $V^-(\cdot)$ and upper value $V^+(\cdot)$ are defined. The lower value $V^-(\cdot)$ indicates the lowest possible outcome of the game (the lowest value of $J(\cdot)$), the upper value $V^+(\cdot)$ is instead the highest one, these values can be defined by giving to a player an advantage respect to the other one. As done in \cite{evans} \cite{reach_avoid_with_dist} \cite{brief_intro} to give a strategic advantage to a player, we impose to him the use of a non-anticipative strategy. This is an advantage because during the game, we assume that each player can choose its own input by knowing the current state of the system (system feedback), and in addition to this information, if a player can use a nonanticipative strategy then it knows also the current input chosen by its opponent \cite{mitchell_time_dep_HJ}.

The lower value of the game is defined by giving to the player that wants to minimize the cost function $J(\cdot)$ the strategic advantage, therefore we define a nonanticipative strategy for $u(\cdot)$ in the following way \cite{evans}:

\begin{equation}
	\label{eq:non_ant_stra_u}
	\begin{split}
		\gamma \in \Gamma \triangleq 
		& \left\{ 
			\alpha : \mathcal{D}(t) \rightarrow \mathcal{U}(t)\,|\,d(r) = \hat{d}(r) \in \mathcal{D}(t)
		\right. \\ 
		& \textrm{for almost every} \;r \in [t,s] \\
		& \implies \alpha[d](r)= \alpha[\hat{d}](r) \\
		& \left. 
			\textrm{for almost every} \; r \in [t,s] 
		\right\}  
	\end{split}
\end{equation}

What the formula says is that, a map $\alpha : \mathcal{D}(t) \rightarrow \mathcal{U}(t)$ is a nonanticipative strategy if for any $s \geq t$, for any $d(\cdot)$ and $\hat{d}(\cdot)$ belonging to $\mathcal{D}$ such that $d(\cdot)$ and $\hat{d}(\cdot)$ coincide almost everywhere on $[t,s]$, the image $\alpha(d(\cdot))$ and $\alpha(\hat{d}(\cdot))$ coincide almost everywhere on $[t,s]$ This restriction means that if player 1 cannot distinguish between input signals $ \alpha (d( \cdot )) $ and $ \alpha (\hat{d}( \cdot ) )$ of player 2 until after time $s$, then player 1 cannot respond differently $( \alpha [d](r) = \alpha [\hat{d}](r))$ to those signals until after time $s$. Or in other words, player 1 cannot respond differently to two different player 2 disturbances until they become different.

Now we can define the lower value of the game $V^-(x,t)$ by allowing $u(\cdot)$ to use $\gamma(\cdot)$:
\begin{equation}
	\label{lower_value_game}
	V^-(x,t) \triangleq \inf_{\gamma(\cdot) \in \Gamma(\cdot)}\sup_{d(\cdot) \in \mathcal{D}(\cdot)}J(x,t, \gamma[d](\cdot), d)
\end{equation}

As mentioned before, in general $V^-(x,t)$ is not the outcome but represents the lower outcome possible of the game started at time $t$ with initial state $x$. However, it is important to highlight that if we assume the use of the information pattern just described (Player 1 uses nonanticipative), then $V^-(x,t)$ is the actual outcome of the game.

Similarly we can define the upper value of the game $V^+(x,t)$, this time Player 2 ($d(\cdot)$) uses a nonanticipative strategy defined as:
\begin{equation}
	\label{eq:non_ant_stra_d}
	\begin{split}
		\delta \in \Delta \triangleq 
		& \left\{ 
			\alpha : \mathcal{U}(t) \rightarrow \mathcal{D}(t)\,|\,u(r) = \hat{u}(r) \in \mathcal{U}(t)
		\right. \\ 
		& \textrm{for almost every} \;r \in [t,s] \\
		& \implies \alpha[u](r)= \alpha[\hat{u}](r) \\
		& \left. 
			\textrm{for almost every} \; r \in [t,s] 
		\right\}  
	\end{split}
\end{equation}

The upper value is then defined by giving to $d(\cdot)$ (player that wants maximizes $J(\cdot)$) the strategic advantage:
\begin{equation}
	\label{upper_value_game}
	V^+(x,t) \triangleq \sup_{\delta(\cdot) \in \Delta(\cdot)}\inf_{u(\cdot) \in \mathcal{U}(\cdot)} J(x,t, u, \delta[u](\cdot))
\end{equation}

For theroretical completeness, we note that $V^-(x,t) \leq V^+(x,t)$ for any $(x,t)$. In those cases in which equality holds, the game is said to have value and $V(x,t)\triangleq V^-(x,t) = V^+(x,t)$ is called the value of the game. \cite{new_paper} \newline

\subsubsection{Differential Game in Reachability Analysis}
Let's try now to understand how differential games theory is related to a reachablity problem, to do that assume we are in the case in which we have a set to reach, namely the target set is a desired set of states. In this case Player 1 tries to bring the system towards the target with its input $u(\cdot)$ and in the meanwhile the disturbance (Player 2) tries to steer the system away the target with its input $d(\cdot)$. This problem can be through as a game between the two players, however in this form, the game theory just described cannot be used since the reward function is a simple boolean value: at the end of the game the system state reaches (Player 1 wins) or not the target set, so we have $J(\cdot)=\left\{0 \textrm{ (win)},1\textrm{ (loss)}\right\}$. To solve this issue we can easily use the Level Set Method that allows to rewrite $J(\cdot)$ as a real function, we will see this later. For the moment assume Player 1 wants to minimize a real value function $J(\cdot)$ and Player 2 tries to maximize it. 
Now we have to understand why the lower and the upper value of the game are important for our purpose. As said before, $V^-(x,t)$ or $V^+(x,t)$ is the actual outcome of the game only if we give a strategic advantage to one player, therefore introducing an information pattern for our game, we can easily define formally the outcome through one of them. In order to consider always the worst possible scenario, the strategic advantage is usually given to the disturbance $d(\cdot)$, then since its aim is to maximize the cost function, the outcome of the game is defined by $V^+(x,t)$.
In a different scenario in which $u(\cdot)$ maximizes and $d(\cdot)$ minimizes $J(\cdot)$ then, since the strategic advantage must be assigned to the disturbance, the outcome this time is given by $V^-(x,t)$. In this paper we always refer to the first scenario. 

As we will see later the BRS is related to the outcome of the game ($V^+(x,t)$), this allows us to solve our reachability problem using the upper value of the game. 
One important result of dynamic programming is that the value functions $V^\pm(x,t)$ are the viscosity solutions of two particular variational inequalities in the form of a Hamilton-Jacobi-Isaacs equation. We will see this formally later, let's now introduce what is a HJI equation, a viscosity solution and how to use the Level Set Method.

\subsection{Hamilton-Jacobi Equation}					
In physics, the Hamilton-Jacobi equation is an alternative formulation of classical mechanics. It is a first-order nonlinear partial differential equation of the form $H(x,u_x(x,\alpha,t),t)+u_t(x,\alpha,t)=K(\alpha,t)$ with independent variables $(x,t)\in \mathbb{R}^n \times \mathbb{R}$ and parameters $\alpha \in \mathbb{R}^n$. It has wide applications in many scientific fields like mechanics. Its solutions determine infinite families of solutions of Hamilton's ordinary differential equations, which are the equations of motion of a mechanical system. In our case, the family of solutions that come from the computation of the HJI PDE and the variational inequality is the viscosity solution. 

\subsection{Viscosity Solution}

Viscosity solution is a concept introduced in \cite{vis_sol}; it is a notion of solution that allows it to be, for example, nowhere differentiable but for which strong uniqueness theorems, stability theorems and general existence theorems are all valid. It is a generalization of the solution to a partial differential equation (PDE). It has been found that the viscosity solution is that kind of solution to use in many applications of PDE's, including for example first order equations arising in dynamic programming (the Hamilton–Jacobi–Bellman equation) or differential games (the Hamilton–Jacobi–Isaacs equation). This means that, given the classical concept of PDE 
\begin{equation}
	\label{diff_eq}
	F(y,u(y), Du(y))=0
\end{equation}

Under the viscosity solution concept, a certain variable $u$ does not need to be everywhere differentiable. There may be points where $Du$ does not exist and yet $u$ satisfies the equation in an appropriate generalized sense.

Taking the main concepts of viscosity solutions from \cite{vis_sol}, let $u$ be a function from O into $\mathbb{R}$  and let $y_0$ belong to $\mathcal{O}$.  The superdifferential and subdifferential of $u$ at $y_0$, denoted, respectively, by $D^{+} u(y_0)$ and $D^{-} u(y_0)$ are the set of points for which, respectively, the following two inequalities hold:

\begin{equation}
	\limsup_{y \to y_0} (u(y)-u(y_0)-p_0 \cdot (y-y_0)) |y-y_0|^{-1} \leq 0
\end{equation}
and

\begin{equation}
	\liminf_{y \to y_0} (u(y)-u(y_0)-p_0 \cdot (y-y_0)) |y-y_0|^{-1} \geq 0
\end{equation}

where $p_0 \cdot (y-y_0)$ is the Euclidean scalar product of $p_0$ and $(y-y_0)$. 
Given these concepts it is possible to define what is a viscosity solution mathematically. 
If $D^+u(y_0)$ and $D^-u(y_0)$ are nonempty at some $x$ and $u$ is differentiate at $x$, a viscosity solution of (\ref{diff_eq}) is a function $u$ belonging to $C(\mathcal{O})$ satisfying the two following conditions:

\begin{equation}
	F(y,u(),p)\leq 0 \forall y \in \mathcal{O}, \forall p \in D^+ u(y)
\end{equation}

\begin{equation}
	F(y,u(),p)\geq 0 \forall y \in \mathcal{O}, \forall p \in D^- u(y)
\end{equation}


\subsection{Level Set Method}
As mentioned before, in order to compute the BRS is necessary to solve a game of kind where the outcome is boolean: the system state either reaches the target set or not. Level Set Method can be used to translate this game into a game of degree, where players share an objective function to optimize. The basic idea of this approach is to encode the boolean outcome through a quantitative function $g(\cdot)$ and compare its value at the end of the game to a threshold value, usually zero, to determine whether the system reached the target set.
The first step is to define a Lipschitz function $g(x(\tau))$, where $x(\tau)$ represents the current system state ($s=\tau\in[t, T]$), such that the target set $R$ corresponds to the sub-zero level set of $g(x(\tau))$, that is, $x(\tau)\in R \Leftrightarrow g(x(\tau)) \leq 0$. We indicated the target set with $R$ (reach) since from now on we suppose that the set contains goal states, namely states to reach. Now we can define the real value cost function of the game $J(\cdot)$, we are not interested in any kind of running cost, therefore we consider only the value of $g(\cdot)$ at the end of a game started at time $s=t$ and initial state $x$:

\begin{equation}
\label{eq:j_level_set}
    J(x, t, u(\cdot), d(\cdot)) = g(x(T))
\end{equation}

Assuming our reachability problem consists only of reaching a target set $R$, then the goal of our control input $u(\cdot)$ is to minimize $J(\cdot)$ while the disturbance $d(\cdot)$ tries to do the opposite. Then giving the strategic advantage to $d(\cdot)$, namely force it to use the non-anticipative strategy $\delta[d](\cdot)$, we define the outcome of the game through its upper value:
\begin{equation}
	\begin{split}
		V^+(x, t) 
		& = \sup_{\delta(\cdot) \in \Delta(\cdot)}\inf_{u(\cdot) \in \mathcal{U}(\cdot)} J(x,t, u, \delta[u](\cdot))    \\
		& = \sup_{\delta(\cdot) \in \Delta(\cdot)}\inf_{u(\cdot) \in \mathcal{U}(\cdot)} g(x(T))
	\end{split}
\end{equation}

However, in practical scenarios, along the trajectory of a dynamical system there may be both goals to reach and obstacles to avoid. The goals to reach can be represented by the target set $R$ as previously done, the set of states to avoid instead, can be defined with another set $A$ (avoid) that contains all the system state $x$ that corresponds to an object collision. This kind of formulation is adopted in \cite{reach_avoid_with_dist} however, we will use another one from \cite{new_paper} in which instead of $A$, its complementary set $K$ is defined. This choice is due to the fact that \cite{new_paper} provides a more advanced problem formulation allowing sets $R$ and $K$ to be time-varying, namely allows to define a reachability problem in which both the goals and the obstacles can vary their positions over time. According to this new formulation, since $R$ can vary over time, also the function $g(\cdot)$ must be time-dependent. The new set $K$ is characterized similarly to $R$ by intoducing another function $h(\cdot)$. Formally: consider the sets $R$, $K$ related respectively to the level sets of two Lipschitz continuous and bounded functions $g: \mathbb{R}^n \times s \in [t, T]\rightarrow \mathbb{R}$, $h: \mathbb{R}^n \times s \in [t, T] \rightarrow \mathbb{R}$, then the two sets can be characterized as:

\begin{equation} 
	\label{R}
	R = \left\{ (x,s) \in \mathbb{R}^n \times [t,T]\,|\, g(x, s) \leq 0 \right\}
\end{equation}
\begin{equation} 
	\label{K}
	K = \left\{(x,s) \in \mathbb{R}^n \times [t,T] \,|\, h(x, s) \leq 0 \right\}
\end{equation}

The most common choice for the functions $g(\cdot)$ and $h(\cdot)$ is to use the distance between the current state $x$ and the set of interested, namely:

\begin{equation}
\label{g}
    g(x, s) =
\left\{
	\begin{array}{ll}
		-d(x, s, R^c)  & \mbox{if } x \in R \\
		d(x, s, R) & \mbox{if } x \in R^c
	\end{array}
\right.
\end{equation}

\begin{equation}
\label{h}
    h(x, s) =
\left\{
	\begin{array}{ll}
		-d(x, s, K^c)  & \mbox{if } x \in K \\
		d(x, s, K) & \mbox{if } x \in K^c
	\end{array}
\right.
\end{equation}

Since $g(\cdot)$, $h(\cdot)$ must be bounded, we will see later why, we can introduce two constants $C_g$ $C_h$ to impose a saturation to the distance functions, or alternatively, we can use the arctangent of the signed distance, in this way the resulting functions are bounded and also globally Lipschitz \cite{reach_avoid_no_dist}. 

In the next section we will see how the value function $V^+(\cdot)$ is formulated when we have both a reach $R$ and a constraints $K$ set, and most importantly how it can be computed in order to calculate the BRS. In the following sections we will refer to the BRS as a reach-avoid set (RAS) to highlight the fact that there is both a set to reach and one to avoid, this last is implicity defined by its complementary $K$, namely $A \triangleq K^c$.
